{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a29dab0",
   "metadata": {},
   "source": [
    "# STA130 Homework 08\n",
    "\n",
    "Please see the course [wiki-textbook](https://github.com/pointOfive/stat130chat130/wiki) for the list of topics covered in this homework assignment, and a list of topics that might appear during ChatBot conversations which are \"out of scope\" for the purposes of this homework assignment (and hence can be safely ignored if encountered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eee301",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Introduction</u></summary>\n",
    "\n",
    "### Introduction\n",
    "    \n",
    "A reasonable characterization of STA130 Homework is that it simply defines a weekly reading comprehension assignment. \n",
    "Indeed, STA130 Homework essentially boils down to completing various understanding confirmation exercises oriented around coding and writing tasks.\n",
    "However, rather than reading a textbook, STA130 Homework is based on ChatBots so students can interactively follow up to clarify questions or confusion that they may still have regarding learning objective assignments.\n",
    "\n",
    "> Communication is a fundamental skill underlying statistics and data science, so STA130 Homework based on ChatBots helps practice effective two-way communication as part of a \"realistic\" dialogue activity supporting underlying conceptual understanding building. \n",
    "\n",
    "It will likely become increasingly tempting to rely on ChatBots to \"do the work for you\". But when you find yourself frustrated with a ChatBots inability to give you the results you're looking for, this is a \"hint\" that you've become overreliant on the ChatBots. Your objective should not be to have ChatBots \"do the work for you\", but to use ChatBots to help you build your understanding so you can efficiently leverage ChatBots (and other resources) to help you work more efficiently.<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Instructions</u></summary>\n",
    "\n",
    "### Instructions\n",
    "    \n",
    "1. Code and write all your answers (for both the \"Pre-lecture\" and \"Post-lecture\" HW) in a python notebook (in code and markdown cells) \n",
    "    \n",
    "> It is *suggested but not mandatory* that you complete the \"Pre-lecture\" HW prior to the Monday LEC since (a) all HW is due at the same time; but, (b) completing some of the HW early will mean better readiness for LEC and less of a \"procrastentation cruch\" towards the end of the week...\n",
    "    \n",
    "2. Paste summaries of your ChatBot sessions (including link(s) to chat log histories if you're using ChatGPT) within your notebook\n",
    "    \n",
    "> Create summaries of your ChatBot sessions by using concluding prompts such as \"Please provide a summary of our exchanges here so I can submit them as a record of our interactions as part of a homework assignment\" or, \"Please provide me with the final working verson of the code that we created together\"\n",
    "    \n",
    "3. Save your python jupyter notebook in your own account and \"repo\" on [github.com](github.com) and submit a link to that notebook though Quercus for assignment marking<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Prompt Engineering?</u></summary>\n",
    "    \n",
    "### Prompt Engineering? \n",
    "    \n",
    "The questions (as copy-pasted prompts) are designed to initialize appropriate ChatBot conversations which can be explored in the manner of an interactive and dynamic textbook; but, it is nonetheless **strongly recommendated** that your rephrase the questions in a way that you find natural to ensure a clear understanding of the question. Given sensible prompts the represent a question well, the two primary challenges observed to arise from ChatBots are \n",
    "\n",
    "1. conversations going beyond the intended scope of the material addressed by the question; and, \n",
    "2. unrecoverable confusion as a result of sequential layers logial inquiry that cannot be resolved. \n",
    "\n",
    "In the case of the former (1), adding constraints specifying the limits of considerations of interest tends to be helpful; whereas, the latter (2) is often the result of initial prompting that leads to poor developments in navigating the material, which are likely just best resolve by a \"hard reset\" with a new initial approach to prompting.  Indeed, this is exactly the behavior [hardcoded into copilot](https://answers.microsoft.com/en-us/bing/forum/all/is-this-even-normal/0b6dcab3-7d6c-4373-8efe-d74158af3c00)...\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc1200",
   "metadata": {},
   "source": [
    "### Marking Rubric (which may award partial credit)\n",
    "- [0.1 points]: All relevant ChatBot summaries [including link(s) to chat log histories if you're using ChatGPT] are reported within the notebook\n",
    "- [0.2 points]: Well-communicated and sensible answers for Question \"2\"\n",
    "- [0.2 points]: Correct code and well-communicated correct answer for Question \"4\" \n",
    "- [0.2 points]: Correct calculations for requested metrics in Question \"6\" \n",
    "- [0.3 points]: Correct and well-communicated explanation of differences for Question \"7\" \n",
    "<!-- - [0.1 points]: Written submission evaluation and enagement confirmation with ChatBot summaries for \"8\", \"10\"-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67686639",
   "metadata": {},
   "source": [
    "## \"Pre-lecture\" HW [*completion prior to next LEC is suggested but not mandatory*]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a53734",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Start a ChatBot session to understand what a *Classification Decision Tree* is: (a) ask the ChatBot to describe the type of problem a *Classification Decision Tree* addresses and provide some examples of real-world applications where this might be particularly useful, and then (b) make sure you understand the difference between how a *Classification Decision Tree* makes *(classification) predictions* versus how *Multiple Linear Regression* makes *(regression) predictions*<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _The first part (a) of this question is looking for you to understand the difference between **classification** and **regression**. The second part (b) of the questions is looking for a fairly high level understanding of the general nature of a decision tree and how it is based on making sequential decisions down the *nodes* of *tree* in order to eventually make a final prediction. This part (b) is essentially the **Classification Decision Tree** analog of \"explain how the **linear form** makes a prediciton in **Multiple Linear Regression** generally speaking\"; namely,\"explain how the **tree** makes a prediciton in a **Classification Decision Tree** generally speaking\"._\n",
    "> \n",
    "> _**If you're struggling with this, it would probably be most helpful to go search for and some images of example decision trees to look at!**_\n",
    "> \n",
    "> - _You may be beginning to realize or will nonetheless eventually come to understand that the sequential decisions at each stage of the **Decision Tree** are **interactions** (in the same manner as **interactions** in **Multiple Linear Regression**.  Once you start to see that and it's making sense to you then you'll increasingly appreciate how **complex** **Decision Tree** models can be, even though they're pretty simple to understand if you just look at one._\n",
    ">\n",
    "> ---\n",
    ">    \n",
    "> _When using chatbots, it's often more effective (and enjoyable) to ask concise, single questions rather than presenting complex, multi-part queries. This approach can help in obtaining clearer and more specific responses (that might be more enjoyable to interact with). You can always ask multi-part questions as a series of additional sequential questions. With this approach, chatbots may not automatically reiterate previously explained concepts. So if you need a refresher or further explanation on a topic discussed earlier, just explicitly request during follow-up interactions._\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c4547a",
   "metadata": {},
   "source": [
    "\n",
    "### **How a Classification Decision Tree Makes Predictions**\n",
    "1. **Tree Structure:**\n",
    "   - A Classification Decision Tree organizes data in a tree-like structure, where each **node** represents a decision point based on a feature, and each **branch** represents an outcome of that decision.\n",
    "   - At the **end of the branches** (leaf nodes), the data is assigned to a category or class (e.g., \"Yes\" or \"No\").\n",
    "\n",
    "2. **Splitting Data:**\n",
    "   - The tree starts at the root node, where it looks at a feature (e.g., \"Is age > 30?\").\n",
    "   - It splits the data into two or more groups based on the answer (e.g., \"Yes\" group for data where age > 30 and \"No\" group for data where age ≤ 30).\n",
    "   - At each step, the algorithm selects the feature and rule that best separate the data into distinct groups. The \"best\" is determined by how much the split improves the purity of the groups (measured by metrics like Gini impurity or information gain).\n",
    "\n",
    "3. **Final Decision:**\n",
    "   - This process continues down the tree until it reaches the leaf nodes, where the data is classified into a specific category.\n",
    "   - For example, if the leaf node represents \"spam emails,\" all the data points reaching that node will be classified as \"spam.\"\n",
    "\n",
    "4. **Key Idea:**\n",
    "   - A decision tree predicts by **breaking down the problem into a series of smaller, simpler decisions**, following the branches until a final classification is made.\n",
    "\n",
    "**Example:**\n",
    "- Imagine you're classifying whether a fruit is an apple or an orange.\n",
    "  - Root Node: \"Is the color orange?\"\n",
    "    - Yes → Go to the orange category.\n",
    "    - No → Next question: \"Is the fruit round?\"\n",
    "      - Yes → Go to the apple category.\n",
    "      - No → Go to the other category.\n",
    "\n",
    "---\n",
    "\n",
    "### **How Multiple Linear Regression Makes Predictions**\n",
    "1. **Core Concept:**\n",
    "   - Multiple Linear Regression works by combining the effects of all the input features to predict a numerical value.\n",
    "   - Each feature is given a weight or importance that reflects how much it influences the final outcome.\n",
    "   - Instead of splitting data like a tree, it looks at the overall relationship between the features and the target value to make a prediction.\n",
    "\n",
    "2. **Influence of Features:**\n",
    "   - It assumes each feature contributes a certain amount to the target. For example:\n",
    "     - In predicting house prices, features like the house’s size, location, and number of bedrooms each contribute to the overall price.\n",
    "   - The contribution of each feature is added up, and the total gives the predicted value.\n",
    "\n",
    "3. **Key Idea:**\n",
    "   - Multiple Linear Regression predicts by **estimating how much each feature contributes to the target and combining those contributions to make a final prediction**.\n",
    "\n",
    "**Example:**\n",
    "- Imagine you’re predicting house prices:\n",
    "  - Size of the house contributes a certain amount (e.g., larger houses cost more).\n",
    "  - The location contributes another amount (e.g., houses in downtown areas are more expensive).\n",
    "  - The regression model combines these factors to predict the total price.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison Between the Two**\n",
    "\n",
    "| Feature                           | Classification Decision Tree                | Multiple Linear Regression                  |\n",
    "|-----------------------------------|---------------------------------------------|---------------------------------------------|\n",
    "| **Purpose**                       | Categorizes data into predefined classes.   | Predicts a continuous numerical value.      |\n",
    "| **Type of Output**                | Discrete categories (e.g., \"Yes\" or \"No\").  | Continuous values (e.g., house price).      |\n",
    "| **How it Works**                  | Follows a sequence of decisions to classify.| Combines the effects of features numerically.|\n",
    "| **Structure**                     | Tree-like structure with nodes and branches.| A mathematical model analyzing overall relationships.|\n",
    "| **Nature of Data**                | Works well with categorical or ordinal data.| Works with numerical and continuous data.    |\n",
    "| **Decision Process**              | Splits data into smaller groups repeatedly. | Looks at the entire dataset holistically.    |\n",
    "| **Example Problem**               | Is this email spam or not?                 | What is the expected price of a house?      |\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Key Differences**\n",
    "- **Focus of a Decision Tree:** It divides data into smaller and smaller groups by asking questions about the features. It builds a hierarchy of decisions to decide the category of an outcome.\n",
    "- **Focus of Linear Regression:** It identifies how much each feature contributes to the outcome and adds these contributions to predict a continuous value.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to elaborate on any specific step, or perhaps visualize how a decision tree works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf97d1",
   "metadata": {},
   "source": [
    "### 2. Continue your ChatBot session and explore with your ChatBot what real-world application scenario(s) might be most appropriately addressed by each of the following *metrics* below: provide your answers and, in your own words, *concisely explain your rationale for your answers.*<br>\n",
    "\n",
    "\n",
    "1. **Accuracy** measures the proportion of true results (both true positives and true negatives) in the population.\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "    \n",
    "2. **Sensitivity** measures the proportion of actual positives that are correctly identified.\n",
    "\n",
    "$$\\text{Sensitivity} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "3. **Specificity** measures the proportion of actual negatives that are correctly identified.\n",
    "\n",
    "$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "4. **Precision** measures the proportion of positive identifications that were actually correct.\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _This question could be restated as, \"Give examples of real-world examples where each of these **metrics** would be particularly useful.\"_\n",
    ">\n",
    "> _The primary objective here is to understand the distinction between each of these **metrics**. The secondary objective is to notice how the nature of the decision-making that each of these **metrics** most naturally supports is very distinct, ideally based on identifying memorable examples that really distinguish between the **metrics**._\n",
    ">\n",
    "> - _Have a look at this (greatly expanded) handy list of additional metrics, formulas, and synonyms at the following [wikipedia page](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) if you want this to get real crazy real fast._\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f21d0e",
   "metadata": {},
   "source": [
    "\n",
    "    1. Accuracy\n",
    "Definition: Accuracy measures the proportion of correctly classified cases (both positives and negatives) out of the total population.\n",
    "When to Use: When both types of errors (false positives and false negatives) are equally important, and the dataset is balanced (i.e., the number of positive and negative cases is similar).\n",
    "Real-World Example: Factory Machine Diagnostics\n",
    "\n",
    "Scenario: A factory uses a diagnostic system to determine whether a machine is \"functional\" (positive) or \"defective\" (negative).\n",
    "Why Accuracy is Relevant: In a balanced dataset where there are roughly equal numbers of functional and defective machines, accuracy provides a good measure of overall performance. For example, if the system identifies 90 out of 100 machines correctly (both functional and defective), its accuracy would be 90%.\n",
    "Limitations: In imbalanced datasets (e.g., if 99% of machines are functional), accuracy can be misleading. A model that predicts all machines as functional would achieve 99% accuracy but fail to detect defective machines.\n",
    "\n",
    "    2. Sensitivity (Recall)\n",
    "Definition: Sensitivity measures the proportion of actual positive cases (e.g., diseased individuals, fraudulent transactions) that are correctly identified by the system.\n",
    "When to Use: When missing a positive case (false negative) has serious consequences and needs to be minimized.\n",
    "Real-World Example: Cancer Screening in Healthcare\n",
    "\n",
    "Scenario: A hospital uses a diagnostic test to screen for breast cancer. The test identifies individuals as either \"positive\" (has cancer) or \"negative\" (does not have cancer).\n",
    "Why Sensitivity is Relevant: Missing a cancer case (false negative) could delay treatment, potentially worsening the patient's condition or leading to death. Sensitivity ensures that most cancer patients are flagged for further testing. For example, a test with 95% sensitivity will detect 95 out of 100 individuals with cancer, minimizing the chance of missing a true case.\n",
    "Trade-offs: High sensitivity often comes at the cost of more false positives (e.g., healthy individuals being flagged for additional testing). This trade-off is acceptable in cases where false negatives (missed detections) are far more critical.\n",
    "\n",
    "    3. Specificity\n",
    "Definition: Specificity measures the proportion of actual negative cases that are correctly identified by the model (e.g., correctly identifying healthy individuals or legitimate transactions).\n",
    "When to Use: When false positives are more problematic than false negatives and need to be minimized.\n",
    "Real-World Example: Fraud Detection in Banking\n",
    "\n",
    "Scenario: A bank’s fraud detection system analyzes transactions to determine whether they are \"fraudulent\" (positive) or \"legitimate\" (negative).\n",
    "Why Specificity is Relevant: If the system incorrectly flags legitimate transactions as fraudulent (false positives), it can cause customer frustration, operational delays, and reputational damage. High specificity ensures that most legitimate transactions are classified correctly, reducing unnecessary disruptions. For example, a model with 98% specificity will correctly identify 98% of legitimate transactions while minimizing false alarms.\n",
    "Trade-offs: High specificity might lead to missing some fraudulent transactions (false negatives), but this is acceptable if the priority is to avoid disrupting genuine customer activity.\n",
    "\n",
    "    4. Precision\n",
    "Definition: Precision measures the proportion of predicted positive cases that are actually correct, emphasizing the reliability of positive predictions.\n",
    "When to Use: When false positives are costly or problematic, precision ensures that the positive predictions are trustworthy.\n",
    "Real-World Example: Email Spam Detection\n",
    "\n",
    "Scenario: An email system classifies incoming messages as either \"spam\" (positive) or \"not spam\" (negative).\n",
    "Why Precision is Relevant: If the system incorrectly classifies important emails as spam (false positives), users may miss critical messages, leading to frustration and a lack of trust in the system. High precision ensures that the emails flagged as spam are indeed spam. For example, a system with 95% precision ensures that 95 out of 100 emails marked as spam are actually spam, minimizing false positives.\n",
    "Trade-offs: High precision might lead to some spam emails not being flagged (false negatives). This trade-off is acceptable in cases where users prioritize not losing important messages over catching all spam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d914e3",
   "metadata": {},
   "source": [
    "### 3. Explore the amazon books dataset, seen previously at the start of the semester, providing some initital standard *exploratory data analysis (EDA)* and data summarization after pre-processing the dataset to meet the requirements below<br>\n",
    "\n",
    " 1. remove `Weight_oz`, `Width`, and `Height` \n",
    " 2. drop all remaining rows with `NaN` entries \n",
    " 3. set `Pub year` and `NumPages` to have the type `int`, and `Hard_or_Paper` to have the type `category`\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _`NaN` entries can't be used in their raw form with the `scikit-learn` methodologies, so we do need to remove them to proceed with our analyses._\n",
    ">     \n",
    "> _Only remove rows with `NaN` entries once you've subset to the columns you're interested in. This will minimize potentially unnecessary data loss..._\n",
    ">\n",
    "> _It would be possible to consider imputing missing data to further mitigate data loss, but the considerations for doing so are more advanced than the level of our course, so we'll not consider that for now._ \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b50624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, recall_score, make_scorer\n",
    "import graphviz as gv\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/pointOfive/STA130_F23/main/Data/amazonbooks.csv\"\n",
    "ab = pd.read_csv(url, encoding=\"ISO-8859-1\")\n",
    "# create `ab_reduced_noNaN` based on the specs above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b326780e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading from URL: HTTP Error 404: Not Found\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_file.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      7\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/point0five/STA130_F23/main/Data/amazonbooks.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m ab \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mISO-8859-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset loaded from URL.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:372\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    371\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    373\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:274\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    562\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    495\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Use local file (update the path to the local file)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath_to_file.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the actual path to the file\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     ab \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mISO-8859-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset loaded from local file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Step 2: Remove specified columns\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_file.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "# Attempt to load the dataset from a URL or fall back to a local file if necessary\n",
    "try:\n",
    "    # Use URL (replace this with a valid URL if the file exists online)\n",
    "    url = \"https://raw.githubusercontent.com/point0five/STA130_F23/main/Data/amazonbooks.csv\"\n",
    "    ab = pd.read_csv(url, encoding=\"ISO-8859-1\")\n",
    "    print(\"Dataset loaded from URL.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading from URL: {e}\")\n",
    "    # Use local file (update the path to the local file)\n",
    "    file_path = \"path_to_file.csv\"  # Replace with the actual path to the file\n",
    "    ab = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "    print(\"Dataset loaded from local file.\")\n",
    "\n",
    "# Step 2: Remove specified columns\n",
    "columns_to_drop = [\"Weight_oz\", \"Width\", \"Height\"]\n",
    "ab_reduced = ab.drop(columns=columns_to_drop, errors=\"ignore\")  # Ignore errors if columns are missing\n",
    "\n",
    "# Step 3: Drop rows with NaN values\n",
    "ab_reduced_noNaN = ab_reduced.dropna()\n",
    "\n",
    "# Step 4: Convert data types\n",
    "# Ensure columns exist before converting data types\n",
    "if \"Pub year\" in ab_reduced_noNaN.columns:\n",
    "    ab_reduced_noNaN[\"Pub year\"] = ab_reduced_noNaN[\"Pub year\"].astype(int)\n",
    "if \"NumPages\" in ab_reduced_noNaN.columns:\n",
    "    ab_reduced_noNaN[\"NumPages\"] = ab_reduced_noNaN[\"NumPages\"].astype(int)\n",
    "if \"Hard_or_Paper\" in ab_reduced_noNaN.columns:\n",
    "    ab_reduced_noNaN[\"Hard_or_Paper\"] = ab_reduced_noNaN[\"Hard_or_Paper\"].astype(\"category\")\n",
    "\n",
    "# Step 5: Summarize the dataset\n",
    "print(\"\\nDataset shape after preprocessing:\", ab_reduced_noNaN.shape)\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(ab_reduced_noNaN.head())\n",
    "\n",
    "print(\"\\nDataset Information:\")\n",
    "print(ab_reduced_noNaN.info())\n",
    "\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(ab_reduced_noNaN.describe(include=\"all\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c225c340",
   "metadata": {},
   "source": [
    "### 4. Create an 80/20 split with 80% of the data as a training set *ab_reduced_noNaN_train* and 20% of the data testing set  *ab_reduced_noNaN_test* using either *df.sample(...)* as done in TUT or using *train_test_split(...)* as done in the previous HW, and report on how many observations there are in the training data set and the test data set.<br><br>Tell a ChatBot that you are about to fit a \"scikit-learn\" *DecisionTreeClassifier* model and ask what the two steps given below are doing; then use your ChatBots help to write code to \"train\" a classification tree *clf* using only the *List Price* variable to predict whether or not a book is a hard cover or paper back book using a *max_depth* of *2*; finally use *tree.plot_tree(clf)* to explain what *predictions* are made based on *List Price* for the fitted *clf* model\n",
    "\n",
    "```python\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "X = ab_reduced_noNaN[['List Price']]\n",
    "```\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _To complete the final 80/20 split of the **observations** in a reproducible way, set a \"random seed\"._ \n",
    "> \n",
    "> - _A single **observation** consists of all the measurements made on a single entity, typically corresponding to a row of a data frame. In **Machine Learning**, a collection of values of interest measured for a single entity is called a \"vector\" and so the **observation** is referred to as a **vector**_.\n",
    ">    \n",
    "> _Asking the ChatBot about \"DecisionTreeClassifier .fit(...)\" can be helpful here..._\n",
    "> \n",
    "> _Should you use the \"ab_reduced_noNaN\" data, or the \"ab_reduced_noNaN_train\" data, or the \"ab_reduced_noNaN_test\" data to initially fit the classification tree? Why?_\n",
    ">    \n",
    "> _You can visualize your decision tree using the `tree.plot_tree(clf)` function shown in the `sklearn` documentation [here](\n",
    "https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#what-is-the-values-array-used-here) and [here](https://scikit-learn.org/stable/modules/tree.html); but, to make it more immediately readible it might be better to use `graphviz`, which is demonstrated in the `sklearn` documentation [here](https://scikit-learn.org/stable/modules/tree.html#alternative-ways-to-export-trees)_ \n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072036a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the predictor (X) and target (y) variables\n",
    "X = ab_reduced_noNaN[['List Price']]  # Predictor: List Price\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']  # Target: Hardcover (1) or Paperback (0)\n",
    "\n",
    "# Split the data into 80% training and 20% testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output the number of observations in training and testing sets\n",
    "print(f\"Number of observations in training set: {len(X_train)}\")\n",
    "print(f\"Number of observations in testing set: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# Initialize and train the decision tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Display the structure of the decision tree\n",
    "tree.plot_tree(clf, feature_names=['List Price'], class_names=['Paperback', 'Hardcover'], filled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9209bd9",
   "metadata": {},
   "source": [
    "The decision tree uses thresholds on List Price to predict whether a book is hardcover or paperback. Each split divides the data into two groups based on the List Price. For example:\n",
    "\n",
    "At the root node:\n",
    "If List Price <= $20, predict paperback.\n",
    "If List Price > $20, move to the next node to refine the decision.\n",
    "Tree Interpretation:\n",
    "\n",
    "At each leaf node, the model predicts the class (Paperback or Hardcover) and provides the probability of that class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8149e79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2ac0e5b",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Continue now...?</u></summary>\n",
    "\n",
    "### Pre-lecture VS Post-lecture HW\n",
    "\n",
    "Feel free to work on the \"Post-lecture\" HW below if you're making good progress and want to continue: for **HW 08** this could be reasonable because, as you'll see, the process of creating and using **classification decision trees** is quite similar to the process for creating and using **multiple linear regression** models. There are differences of course, such as how there is **coefficient hypothesis testing** in **multiple linear regression** and **confusion matrices** in **classification decision trees**, and so on. But you would very likely be able to leverage the silarities to make a lot of progress with **classification decision trees** based on your experience with **multiple linear regression**.\n",
    "    \n",
    "*The benefits of continue would are that (a) Consolidate the knowledge already learned and integrate it comprehensively. (b) Let you build experience interacting with ChatBots (and understand their strengths and limitations in this regard)... it's good to have sense of when using a ChatBot is the best way to figure something out, or if another approach (such as course provided resources or a plain old websearch for the right resourse) would be more effective*\n",
    "    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e7d3e",
   "metadata": {},
   "source": [
    "## \"Post-lecture\" HW [*submission along with \"Pre-lecture\" HW is due prior to next TUT*]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b3ce1f",
   "metadata": {},
   "source": [
    "### 5. Repeat the previous problem but this time visualize the *classification decision tree* based on the following specifications below; then explain generally how predictions are made for the *clf2* model<br>\n",
    "\n",
    "1. `X = ab_reduced_noNaN[['NumPages', 'Thick', 'List Price']]`\n",
    "2. `max_depth` set to `4`\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> - _Use the same **train/test split** dataset used so far_\n",
    "> - _Train the **classification decision tree** `clf2` using **predictor variables** `NumPages`, `Thick` and `List Price`_ \n",
    "> - _Again **predict** whether or not a book is hard cover book or a paper back book_\n",
    "> - _You can visualize your decision tree using the `tree.plot_tree(clf)` function shown in the `sklearn` documentation [here](\n",
    "https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#what-is-the-values-array-used-here) and [here](https://scikit-learn.org/stable/modules/tree.html); but, to make it more immediately readible it might be better to use `graphviz`, which is demonstrated in the `sklearn` documentation [here](https://scikit-learn.org/stable/modules/tree.html#alternative-ways-to-export-trees)_\n",
    ">\n",
    "> _If you are interested in how to find the best `max_depth` for a tree, ask ChatBot about \"GridSearchCV\"_\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# Step 1: Define predictors (X) and target (y) for training\n",
    "X_train = ab_reduced_noNaN_train[['NumPages', 'Thick', 'List Price']]\n",
    "y_train = pd.get_dummies(ab_reduced_noNaN_train[\"Hard_or_Paper\"])['H']\n",
    "\n",
    "# Step 2: Train a new decision tree with max_depth=4\n",
    "clf2 = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Visualize the decision tree using tree.plot_tree\n",
    "tree.plot_tree(clf2, feature_names=['NumPages', 'Thick', 'List Price'], \n",
    "               class_names=['Paper', 'Hard'], filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a1d19f",
   "metadata": {},
   "source": [
    "Explain Predictions Made by clf2\n",
    "The decision tree uses thresholds on the variables NumPages, Thick, and List Price to classify whether a book is hardcover or paperback.\n",
    "Example Interpretations:\n",
    "If List Price <= 25, the tree might predict a paperback.\n",
    "If Thick == 1 and List Price > 30, the tree might predict a hardcover.\n",
    "At each decision node, the tree splits the data to maximize class separation until the maximum depth (4 in this case) is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c38051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define predictors (X) and target (y) for testing\n",
    "X_test = ab_reduced_noNaN_test[['NumPages', 'Thick', 'List Price']]\n",
    "y_test = pd.get_dummies(ab_reduced_noNaN_test[\"Hard_or_Paper\"])['H']\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf2.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the model on the test set: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3791e9a2",
   "metadata": {},
   "source": [
    "The clf2 model uses the following logic:\n",
    "\n",
    "At Each Node:\n",
    "It checks conditions (e.g., List Price <= 25, Thick == 1, etc.).\n",
    "Based on the condition, it splits the data further into sub-nodes.\n",
    "At Leaf Nodes:\n",
    "The tree assigns a final classification (hardcover or paperback).\n",
    "Each leaf node includes:\n",
    "The predicted class (e.g., hardcover or paperback).\n",
    "The proportion of samples classified in that class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d889386a",
   "metadata": {},
   "source": [
    "### 6. Use previously created *ab_reduced_noNaN_test* to create confusion matrices for *clf* and *clf2*. Report the sensitivity, specificity and accuracy for each of the models<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _Hopefully you immediately thought to ask ChatBot to help you with this problem, but if you did you should take time to make sure you're clear about the key components of what the ChatBot is providing for you. You might want to know_\n",
    "> - _what is a \"positive\" and what is a \"negative\"_\n",
    "> - _how to read an `sklearn` confusion matrix_\n",
    "> - _what leads to TP, TN, FP, and FN_\n",
    "> - _whether `y_true` or `y_pred` go first in the `confusion_matrix` function_   \n",
    ">\n",
    "> _Have the visualizations you make use decimal numbers with three signifiant digits, such as `0.123` (and not as percentages like `12.3%`), probably based on `np.round()`_\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "       \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c3a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Step 1: Predict using clf\n",
    "y_pred_clf = clf.predict(X_test)\n",
    "\n",
    "# Step 2: Predict using clf2\n",
    "y_pred_clf2 = clf2.predict(X_test)\n",
    "\n",
    "# Step 3: Compute confusion matrices for clf and clf2\n",
    "cm_clf = confusion_matrix(y_test, y_pred_clf)\n",
    "cm_clf2 = confusion_matrix(y_test, y_pred_clf2)\n",
    "\n",
    "# Print confusion matrices\n",
    "print(\"Confusion Matrix for clf (List Price only):\")\n",
    "print(cm_clf)\n",
    "\n",
    "print(\"\\nConfusion Matrix for clf2 (NumPages, Thick, List Price):\")\n",
    "print(cm_clf2)\n",
    "\n",
    "# Step 4: Define a function to calculate metrics from confusion matrix\n",
    "def calculate_metrics(cm):\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    sensitivity = TP / (TP + FN)  # Recall\n",
    "    specificity = TN / (TN + FP)\n",
    "    return accuracy, sensitivity, specificity\n",
    "\n",
    "# Step 5: Calculate metrics for both models\n",
    "accuracy_clf, sensitivity_clf, specificity_clf = calculate_metrics(cm_clf)\n",
    "accuracy_clf2, sensitivity_clf2, specificity_clf2 = calculate_metrics(cm_clf2)\n",
    "\n",
    "# Step 6: Display results with 3 decimal places\n",
    "print(\"\\nMetrics for clf (List Price only):\")\n",
    "print(f\"Accuracy: {accuracy_clf:.3f}, Sensitivity: {sensitivity_clf:.3f}, Specificity: {specificity_clf:.3f}\")\n",
    "\n",
    "print(\"\\nMetrics for clf2 (NumPages, Thick, List Price):\")\n",
    "print(f\"Accuracy: {accuracy_clf2:.3f}, Sensitivity: {sensitivity_clf2:.3f}, Specificity: {specificity_clf2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130884af",
   "metadata": {},
   "source": [
    "Confusion Matrix:\n",
    "Use the matrix to identify the number of correctly and incorrectly classified instances for both models.\n",
    "Compare the performance of clf and clf2 based on the metrics.\n",
    "Metrics:\n",
    "Accuracy: Higher values indicate better overall performance.\n",
    "Sensitivity (Recall): Higher values mean the model is better at identifying hardcovers.\n",
    "Specificity: Higher values mean the model is better at identifying paperbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1d470c",
   "metadata": {},
   "source": [
    "y_test represents the true class labels from the test set.\n",
    "y_pred_clf and y_pred_clf2 are the predicted labels from clf and clf2, respectively.\n",
    "Use these results to identify whether adding more predictors (NumPages and Thick) improves the model compared to using only List Price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72576c25",
   "metadata": {},
   "source": [
    "### 7. Explain in three to four sentences what is causing the differences between the following two confusion matrices below, and why the two confusion matrices above (for *clf* and *clf2*) are better<br>\n",
    "\n",
    "```python\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(ab_reduced_noNaN_train.your_actual_outcome_variable, \n",
    "                     clf.predict(ab_reduced_noNaN_train[['List Price']]), \n",
    "                     labels=[0, 1]), display_labels=[\"Paper\",\"Hard\"]).plot()\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(ab_reduced_noNaN_train.your_actual_outcome_variable, \n",
    "                     clf.predict(\n",
    "                         ab_reduced_noNaN_train[['NumPages','Thick','List Price']]), \n",
    "                     labels=[0, 1]), display_labels=[\"Paper\",\"Hard\"]).plot()\n",
    "```\n",
    "\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1024e7",
   "metadata": {},
   "source": [
    "1. Differences Between clf and clf2\n",
    "clf:\n",
    "Trained using only List Price as the predictor.\n",
    "This model may capture some relationship between the price of a book and whether it’s hardcover or paperback. However, it may miss patterns influenced by other variables like the number of pages or thickness, leading to less accurate predictions.\n",
    "clf2:\n",
    "Trained using NumPages, Thick, and List Price as predictors.\n",
    "This model uses additional information (number of pages and thickness) that likely correlates with whether a book is hardcover or paperback. For example:\n",
    "Hardcover books are often thicker and have more pages than paperbacks.\n",
    "Including these variables helps the model learn more nuanced decision boundaries, improving classification performance.\n",
    "2. Why the Confusion Matrices are Different\n",
    "Limited Features in clf:\n",
    "Using only List Price restricts the model's ability to classify books accurately, especially if the relationship between price and cover type is not strong. This may result in higher misclassification rates (i.e., more false positives and false negatives).\n",
    "Additional Features in clf2:\n",
    "Including NumPages and Thick provides more information, allowing the model to make more accurate predictions. For example:\n",
    "A thick book with many pages is more likely to be hardcover regardless of price.\n",
    "The combined use of predictors results in better separation between the classes.\n",
    "3. Why clf2 is Likely Better\n",
    "Improved Sensitivity and Specificity:\n",
    "By incorporating additional predictors, clf2 is likely to achieve higher sensitivity (better at detecting hardcover books) and specificity (better at identifying paperback books).\n",
    "Lower Misclassification Rates:\n",
    "With access to more relevant data (NumPages and Thick), clf2 can reduce both false positives and false negatives compared to clf.\n",
    "4. How to Compare Using Metrics\n",
    "After generating the confusion matrices, compare the following metrics for clf and clf2:\n",
    "\n",
    "Sensitivity:\n",
    "Measures how well the model identifies hardcover books.\n",
    "Specificity:\n",
    "Measures how well the model identifies paperback books.\n",
    "Accuracy:\n",
    "Measures the overall proportion of correct predictions.\n",
    "These metrics will demonstrate that clf2 outperforms clf due to the inclusion of additional predictive features.\n",
    "\n",
    "5. Example Summary\n",
    "In 3–4 sentences:\n",
    "\n",
    "The differences between the confusion matrices for clf and clf2 arise because clf uses only List Price, while clf2 includes additional predictors (NumPages and Thick). These added features allow clf2 to capture more complex relationships that influence whether a book is hardcover or paperback.\n",
    "As a result, clf2 achieves higher sensitivity and specificity, reducing false positives and false negatives compared to clf.\n",
    "This demonstrates that including relevant features improves model performance, making clf2 a better classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c13a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b0560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4542bec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224c18ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "526939c2",
   "metadata": {},
   "source": [
    "\n",
    "### 8. Read the paragraphs in *Further Guidance* and ask a ChatBot how to visualize *feature Importances* available for *scikit-learn* *classification decision trees*; do so for *clf2*;  and use *.feature_names_in_* corresponding to *.feature_importances_* to report which *predictor variable* is most important for making predictions according to *clf2*<br>\n",
    "\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "  \n",
    "> The way a **classification decision tree** is fit is that at each step in the construction process of adding a new **decision node splitting rule** to the current **tree structure**, all possible **decision rules** for all possible **predictor variables** are considered, and the combination that improves the **prediction** the most (as measured by the criterion of either \"Gini impurity\" or \"Shannon entropy\") and in accordance with the rules of the decision tree (such as the `max_depth` argument) is added to the **classification decision tree**.  Thus overall \"criterion\" noted above improves with each new **decision node splitting rule**, so the improvement can thus be tracked and the improvement contributions attributed to the **feature** upon which the **decision node splitting rule** is based.  This means the relative contribution of each **predictor variable** to the overall explanatory power of the model can be calculated, and this is what the `.feature_importances_` attribute does. \n",
    ">\n",
    "> Compared to the simplicity of understanding how different **covariates** contribute towards the final **predicted values** of **multiple linear regression models** (by just reading off the equation to see how predictions work), the the complexity of how all the different **features** interact and combine to together to create the final **predictions** from **classification decision trees** can be staggering. But the so-called **feature importance** heuristics allows us to judge how relatively important the overall contributions from different features are in the final decision tree predictions. Now we just need to be sure we're not **overfitting** our **classification decision trees** since they can be so **complex**. Fortunately, the \"GridSearchCV\" methodology mentioned in regards to finding the best `max_depth` setting for a tree is going to provide a general answer to the challenge of complexity and **overfitting** in **machine learning models** that is not too hard to understand (and which you might already have some guesses or a hunch about). \n",
    "> \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "       \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a660d",
   "metadata": {},
   "source": [
    "1. What are Feature Importances?\n",
    "Definition: Feature importance scores represent how much each predictor variable contributes to improving the model's performance during splits.\n",
    "How It's Calculated: Decision tree algorithms calculate feature importance based on how much each variable reduces impurity (e.g., Gini impurity or entropy) at each split.\n",
    "Usefulness: The higher the score for a variable, the more it influences predictions.\n",
    "2. Access Feature Importances for clf2\n",
    "The DecisionTreeClassifier object clf2 has an attribute .feature_importances_ that contains the importance scores for each predictor variable. These scores correspond to the variables used in training (NumPages, Thick, and List Price).\n",
    "\n",
    "3. Visualize and Analyze Feature Importances\n",
    "We will:\n",
    "\n",
    "Print the importance scores for each variable.\n",
    "Use a bar plot to visualize the relative importance of the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b14e8c5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Step 1: Extract feature importances from clf2\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m feature_importances \u001b[38;5;241m=\u001b[39m \u001b[43mclf2\u001b[49m\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[1;32m      5\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumPages\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThick\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mList Price\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Step 2: Display feature importance values\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf2' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Extract feature importances from clf2\n",
    "feature_importances = clf2.feature_importances_\n",
    "feature_names = ['NumPages', 'Thick', 'List Price']\n",
    "\n",
    "# Step 2: Display feature importance values\n",
    "print(\"Feature Importances for clf2:\")\n",
    "for name, importance in zip(feature_names, feature_importances):\n",
    "    print(f\"{name}: {importance:.3f}\")\n",
    "\n",
    "# Step 3: Visualize feature importances as a bar plot\n",
    "plt.bar(feature_names, feature_importances, color='skyblue')\n",
    "plt.title(\"Feature Importances in clf2\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb7d92c",
   "metadata": {},
   "source": [
    "### 9. Describe the differences of interpreting coefficients in linear model regression versus feature importances in decision trees in two to three sentences<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _Linear model regression predicts continuous real-valued averages for a given configuration of covariate values (or, feature values, if we're using machine learning terminology instead of statistical terminology), whereas a binary classification model such as a binary classification tree predicts 0/1 (\"yes\" or \"no\") outcomes (and gives the probability of a 1 \"yes\" (or \"success\") outcome from which a 1/0 \"yes\"/\"no\" prediction can be made; but, this is not what is being asked here. This question is asking \"what's the difference in the way we can interpret and understand how the predictor variables influence the predictions in linear model regression based on the coefficients versus in binary decision trees based on the Feature Importances?\"_\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa628e15",
   "metadata": {},
   "source": [
    "Differences Between Linear Model Regression Coefficients and Decision Tree Feature Importances\n",
    "\n",
    "Linear Model Regression Coefficients:\n",
    "Coefficients in linear regression represent the direct relationship between a predictor and the target outcome, assuming all other predictors remain constant.\n",
    "They indicate the magnitude and direction of change: for example, a coefficient of 3 for a predictor means that for every unit increase in the predictor, the outcome increases by 3 (if the relationship is positive).\n",
    "\n",
    "Decision Tree Feature Importances:\n",
    "Feature importances in decision trees indicate how much each predictor contributes to reducing uncertainty (e.g., Gini impurity or entropy) across all splits in the tree.\n",
    "They do not specify the direction or exact effect on the outcome, only the relative importance of the predictor in making decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e9ff83",
   "metadata": {},
   "source": [
    "Linear regression coefficients explicitly quantify the magnitude and direction of a predictor’s influence on the target, assuming a linear relationship. In contrast, decision tree feature importances measure the overall contribution of a predictor to reducing uncertainty in the tree, without specifying the direction or magnitude of its effect. Linear regression is more interpretable for simple relationships, while decision trees are better suited for capturing complex, non-linear interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6fb0cb",
   "metadata": {},
   "source": [
    "### 10. Have you reviewed the course wiki-textbook and interacted with a ChatBot (or, if that wasn't sufficient, real people in the course piazza discussion board or TA office hours) to help you understand all the material in the tutorial and lecture that you didn't quite follow when you first saw it?<br>\n",
    "  \n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    ">  _Here is the link of [wiki-textbook](https://github.com/pointOfive/stat130chat130/wiki) in case it gets lost among all the information you need to keep track of_  : )\n",
    "> \n",
    "> _Just answering \"Yes\" or \"No\" or \"Somewhat\" or \"Mostly\" or whatever here is fine as this question isn't a part of the rubric; but, the midterm and final exams may ask questions that are based on the tutorial and lecture materials; and, your own skills will be limited by your familiarity with these materials (which will determine your ability to actually do actual things effectively with these skills... like the course project...)_\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15becb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2436778",
   "metadata": {},
   "source": [
    "# Recommended Additional Useful Activities [Optional]\n",
    "\n",
    "The \"Ethical Profesionalism Considerations\" and \"Current Course Project Capability Level\" sections below **are not a part of the required homework assignment**; rather, they are regular weekly guides covering (a) relevant considerations regarding professional and ethical conduct, and (b) the analysis steps for the STA130 course project that are feasible at the current stage of the course \n",
    "\n",
    "<br>\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Ethical Professionalism Considerations</u></summary>\n",
    "\n",
    "### Ethical Professionalism Considerations\n",
    "\n",
    "- Discuss with a ChatBox about consent and data collection for training models.\n",
    "    - Discuss the ethics of data collection for training decision trees, particularly the need for informed consent when personal data is involved.\n",
    "    - Evaluate the role of regulatory frameworks in ensuring ethical data collection practices.\n",
    "- Discuss with a ChatBox about accountability in automated decision-making.\n",
    "    - Address the challenges of holding systems and their developers accountable when decision trees lead to adverse outcomes.\n",
    "    - Explore legal and ethical frameworks for responsibility when automated decisions go wrong.\n",
    "- Discuss with a ChatBox about transparency and explainability in classification models.\n",
    "    - Discuss the importance of model transparency, particularly when using decision trees in sectors like healthcare or criminal justice.\n",
    "    - Explore methods to enhance the explainability of decision trees, such as visualization techniques and simplified decision paths.\n",
    "- Discuss with a ChatBox about impact of misclassifications in critical applications.\n",
    "    - Examine the consequences of false positives and false negatives in decision tree outcomes, using confusion matrices to highlight these issues.\n",
    "    - Discuss ethical responsibilities when deploying classifiers in high-stakes fields like medicine or law enforcement.\n",
    "    \n",
    "</details>    \n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Current Course Project Capability Level</u></summary>\n",
    "\n",
    "**Remember to abide by the [data use agreement](https://static1.squarespace.com/static/60283c2e174c122f8ebe0f39/t/6239c284d610f76fed5a2e69/1647952517436/Data+Use+Agreement+for+the+Canadian+Social+Connection+Survey.pdf) at all times.**\n",
    "\n",
    "Information about the course project is available on the course github repo [here](https://github.com/pointOfive/stat130chat130/tree/main/CP), including a draft [course project specfication](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F23_course_project_specification.ipynb) (subject to change). \n",
    "- The Week 01 HW introduced [STA130F24_CourseProject.ipynb](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F24_CourseProject.ipynb), and the [available variables](https://drive.google.com/file/d/1ISVymGn-WR1lcRs4psIym2N3or5onNBi/view). \n",
    "- Please do not download the [data](https://drive.google.com/file/d/1mbUQlMTrNYA7Ly5eImVRBn16Ehy9Lggo/view) accessible at the bottom of the [CSCS](https://casch.org/cscs) webpage (or the course github repo) multiple times.\n",
    "    \n",
    "\n",
    "> ### NEW DEVELOPMENT<br>New Abilities Achieved and New Levels Unlocked!!!    \n",
    "> \n",
    "> And with that, ALL LEVELS unlocked! \n",
    ">\n",
    "> CONGRATS, YOU LEGENDS! 🎉\n",
    ">\n",
    "> You’ve battled through the wild jungles of deadlines, defeated the mighty Homework Beasts, and climbed the towering Mount Procrastination. And guess what? YOU MADE IT TO THE TOP! 🏔️\n",
    "> \n",
    "> Take a bow, grab a treat, and enjoy the sweet, sweet taste of freedom(**just for now , because you still have to finish the project! But you are almost done!**). You’ve earned it. Now go out there and celebrate like the absolute rockstars you are! 🌟💪\n",
    ">\n",
    "\n",
    "    \n",
    "### Current Course Project Capability Level    \n",
    "    \n",
    "I mean, the **course project** is basically, like, essentially now.\n",
    "    \n",
    "- Will you be doing any **classification decision trees** stuff for the course project?\n",
    "    - You could consider making some [partial dependency plots](https://scikit-learn.org/stable/modules/partial_dependence.html) if so...\n",
    "    - those might provide an interesting analysis in addition to **tree structure** visualizations, **confusion matrices**, **feature importances**. and the standard \"in-sample versus out-of-sample\" **train-test validation** analysis that would be expected in a **machine learning context**\n",
    "    \n",
    "- You could see if there are any interesting columns that might make for a potentially interesting **classification decision tree** analysis\n",
    "    - You wouldn't have to though...\n",
    "    - But if you did you'd want to be able to articulate and explain why what you're doing with **classification decision trees** is appropriate and enlightening\n",
    "\n",
    "- Anyway, I guess that just leaves reviewing all the statistical techniques covered in STA130, and considering integrating them holistically into your project!\n",
    "    \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524759d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
